## Limitations and Trends (2 days)

# Limitations
* [Failures of Gradient-Based Deep Learning](https://arxiv.org/abs/1703.07950)
* [Intriguing Properties of Neural Networks](http://arxiv.org/abs/1312.6199)
* [Deep neural nets are easily fooled: High confidence predictions for unrecognizable images](http://arxiv.org/abs/1412.1897)
* [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
* [Measuring Invariances in Deep Networks](http://ai.stanford.edu/~ang/papers/nips09-MeasuringInvariancesDeepNetworks.pdf)
* [Adversarial Examples in the Physical World](https://arxiv.org/abs/1607.02533)
* [Houdini: Fooling Deep Structured Prediction Networks](https://arxiv.org/abs/1707.05373)
* [Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks](https://arxiv.org/abs/1711.00350)
* [A Surprising Linear Relationship Predicts Test Performance in Deep Networks](https://arxiv.org/abs/1807.09659)
* [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530)
* [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078)
* Natural Language Processing
    * [Workshop on New Forms of Generalization in Deep Learning and Natural Language Processing](https://newgeneralization.github.io/)
        * [Deep RNNs Encode Soft Hierarchical Syntax](https://arxiv.org/abs/1805.04218)
    * [Evaluating the Ability of LSTMs to Learn Context-Free Grammars](https://arxiv.org/abs/1811.02611)
    * [Breaking NLI Systems with Sentences that Require Simple Lexical Inferences](https://arxiv.org/abs/1805.02266)
* https://sites.google.com/view/nips2018causallearning/home

# Trends
* Memory-Augmented Networks
    * Neural Turing Machines
    * Memory Networks
    * Differentiable Neural Computers
    * [Learning to Transduce with Unbounded Memory](https://arxiv.org/abs/1506.02516)
* GANs
* Attention
* Deep Reinforcement Learning
* Neural Style Transfer
* Meta-learning, Few-Shot Learning
* RNNs, CNNs, and the Transformer
    * https://www.reddit.com/r/MachineLearning/comments/9jwqip/d_convolutional_sequence_to_sequence_learning/
* Variational inference
* Graph Neural Networks
* Explaining Generalization of Neural Networks
* Capsule Networks
